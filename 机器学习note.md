# 机器学习note:  

读《机器学习》----Peter Flach  笔记。

## 零 绪论 机器学习概述：  

1、机器学习的一般性定义：**机器学习是对*依据经验*提升自身性能或丰富自身知识的各种算法和系统的系统性研究。**  

 * 针对以上概念，以垃圾邮件分类为例：  
   * “经验”对应一组正确标注的训练数据。
   * “性能”对应于识别垃圾邮件的能力。  
* 同时，在某些任务中，机器学习的目的可能不是针对特定任务取得性能提升，而是在整体上使知识得到提升。  
* 在训练数据上取得优异性能只是手段，而非目的。------否则就会因为一味追求在训练数据上的性能，而造成---**”过拟合（overfitting）“**。    

--------------------------------------------------------------------

> ### 数学背景1：概率论  
>
> * 概率**涉及描述“事件”结果的随机变量。这里的“事件”通常是以“假设”的形式出现的，因此需要用估计的方法来得到其概率。
>
> * “**条件概率**$P(A|B)$“刻画的是当事件B发生时，事件A发生的概率。
>   * $P(A,B)=P(A|B)P(B)=P(A)P(B|A)$
>   * $P(A|B)=\frac{P(A,B)}{P(B)}$ 
>   * “**贝叶斯公式**”---$P(A|B)=\frac{P(A)P(B|A)}{P(B)}$  
> * 事件A与事件B独立：即事件B发生与否不影响事件A的发生。  
>   * $P(A|B)=P(A)$
>   * $P(A,B)=P(A)P(B)$  
> * 事件的**“几率”** ---指该事件的发生概率与不发生概率的比值。设发生概率为$p$.  
>   * 几率为：$o=\frac{p}{1-p}$  
>   * $p=\frac{o}{o+1}$  
>   * **有时在实践中，使用几率比使用概率更为方便，因为他是用乘法尺度来表达的。**将量个几率相乘是将本质上同一的信息统计了两次。

-------------------------------------------------------------------

*贝叶斯分类的好处在于进一步的证据可以在原有基础上使用。*  

**在检测垃圾邮件的例子中，目前需要做的是通过分析带有正确标注信息的电子邮件训练集，发现样本特征与其所属类别之间的联系----该联系称为“模型”。**  

2、“任务”、“模型”、“特征”是机器学习的三大“原料”。如下图所示：  

![1531548572074](ml1)

* 要完成一个任务，需要建立从用特征描述的数据到输出的恰当映射（即模型）。
* 学习问题的中心任务是研究如何从训练数据中获取这样的映射（模型）。
* “任务”和“学习问题”的不同在于：  
  * “任务”是通过模型来完成的。
  * “学习问题”是通过能够产生模型的学习算法来解决的。
  * 也就是学习问题是需要学习算法学习出模型来完成任务。  

> ***机器学习所关注的问题是使用正确的特征来构建正确的模型，以完成既定的任务***。  

--------------------------------------------------------------

## 一 机器学习的构成要素：  

*模型赋予了机器学习领域以多样性，而特征和任务则为其带来了某种程度的一致性。*  

### 任务：可通过机器学习解决的问题    

**任务（task）是对我们所期望解决的、与问题域对象有关的问题的一种抽象表示**。

* 有时完全抛弃离散类别的概念，而预测出具体值更为自然。这样的任务称为----**回归（regression）**。  
  * 本质上是依据标注有函数输出真值的训练样本集来学习一个实值函数。  

分类和回归的共同假设是可以获得由带有类别真值或函数真值标注的样本所构成的训练集。这样的学习称之为----**有监督学习（supervised learning）**。  

而相对应的对数据进行分组，但不利用与“组”有关的任何先验信息的任务称为**聚类（clustering）**，这样的学习称之为----**无监督学习（unsurpervised learning）**。  

* 通过无监督学习的方式，可以从数据中学到许多其他类型的模式。如*关联规则（association）*。    
* 按照模型的输出是否含有目标变量来划分模型：  
  * 预测性模型（predicative model）  
  * 描述性模型（descriptive model）  

机器学习分类可用下表来表示：  

|            | 预测性模型 |           描述性模型           |
| :--------: | :--------: | :----------------------------: |
| 有监督学习 | 分类、回归 | 子群发现（subgroup discovery） |
| 无监督学习 | 预测性聚类 |    描述性聚类、关联规则发现    |



### 模型：机器学习的输出    

对三组模型进行讨论：  

* 几何模型（geometric model）
* 概率模型（probabilistic model）
* 逻辑模型（logical model）  

#### 几何模型：

* **实例空间（instance space）**：所以可能的或可描述的实例（即样本）所构成的集合。
* 几何模型则是借助于一些几何概念（如线、平面及距离）直接在实例空间中构建的。
* **线性可分（linearly seperable）**：如果存在某个线性决策面能够将两类样本分离，称所给的数据集为线性可分的。  
* **距离（distance）**：如果两个实例之间的距离很小，则意味着二者的特征值相似。在笛卡尔坐标系中，距离可用*欧式距离（Euclidean distance）*来度量：$\sqrt{\sum^d_{i=1}(x_i-y_i)^2}$。  
  * 最近邻分类器（nearest-neighbor classifier）：为确定一个新实例所属的类别，可以首先从内存中获取与该实例最相似的训练实例（即与待分类实例欧式距离最小的那些训练实例），并将这些训练实例的类别赋予该实例）。

#### 概率模型：  

比如贝叶斯分类器。  

* 令$X$为已知变量（如实例的特征）。
* $Y$为我们感兴趣的**目标变量（target variable）**（如实例所属的类别）。
* 机器学习最关键的问题是如何对X和Y之间的以来关系进行建模。-----*在统计学中，假设这些变量的观测值由一些潜在的随机过程按照某个明确定义、但却未知的概率分布所产生。*希望通过数据来获得与该分布有关的更多信息。  

> “后验概率（posterior probability）”---条件概率的使用发生在观测到特征X之后。    

* 依据X和后验概率$P(Y|X)$来预测Y的方法称为**决策规则（decision rule）。    

> “先验概率（prior probability）”---P(Y)。  在分类问题中，其传达的信息是在观测到数据X之前每个类别的数据出现的可能性。
>
> “似然函数”---$P(X|Y)$  
>
> 三者关系：（通过贝叶斯公式）  
>
> $$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$$  
>
> “最大后验概率（Maximum A Posteriori，MAP）决策规则”：  
>
> $$y_{MAP}=arg max_YP(Y|X)=arg max_Y\frac{P(X|Y)P(Y)}{P(X)}=arg max_YP(X|Y)P(Y)$$  
>
> 如果先验分布为均匀分布，则上式可以简化为“最大似然（Maximum Likelihood，ML）决策规则：  
>
> $$y_{MAP}=arg max_YP(X|Y)$$    
>
> **如果忽略先验分布或假定他为均匀分布，则使用似然，否则使用后验概率。**  
>
> 如果问题域中仅涉及两个类别，则使用后验概率比或似然比会更方便。
>
> 

#### 逻辑模型：  

这种模型很容易使用一个树形结构来表示----特征树（feature tree）---利用特征以迭代的当时不短划分实例空间。



### 特征：机器学习的马达  

模型的质量直接由特征决定的，特征可被视为一种易于在任意实例上度量的测度。  

* 特征