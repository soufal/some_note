# 机器学习note:  

读《机器学习》----Peter Flach  笔记。

[TOC]



## 零 绪论 机器学习概述：  

1、机器学习的一般性定义：**机器学习是对*依据经验*提升自身性能或丰富自身知识的各种算法和系统的系统性研究。**  

 * 针对以上概念，以垃圾邮件分类为例：  
   * “经验”对应一组正确标注的训练数据。
   * “性能”对应于识别垃圾邮件的能力。  
* 同时，在某些任务中，机器学习的目的可能不是针对特定任务取得性能提升，而是在整体上使知识得到提升。  
* 在训练数据上取得优异性能只是手段，而非目的。------否则就会因为一味追求在训练数据上的性能，而造成---**”过拟合（overfitting）“**。    

--------------------------------------------------------------------

> ### 数学背景1：概率论  
>
> * 概率**涉及描述“事件”结果的随机变量。这里的“事件”通常是以“假设”的形式出现的，因此需要用估计的方法来得到其概率。
>
> * “**条件概率**$P(A|B)$“刻画的是当事件B发生时，事件A发生的概率。
>   * $P(A,B)=P(A|B)P(B)=P(A)P(B|A)$
>   * $P(A|B)=\frac{P(A,B)}{P(B)}$ 
>   * “**贝叶斯公式**”---$P(A|B)=\frac{P(A)P(B|A)}{P(B)}$  
> * 事件A与事件B独立：即事件B发生与否不影响事件A的发生。  
>   * $P(A|B)=P(A)$
>   * $P(A,B)=P(A)P(B)$  
> * 事件的**“几率”** ---指该事件的发生概率与不发生概率的比值。设发生概率为$p$.  
>   * 几率为：$o=\frac{p}{1-p}$  
>   * $p=\frac{o}{o+1}$  
>   * **有时在实践中，使用几率比使用概率更为方便，因为他是用乘法尺度来表达的。**将量个几率相乘是将本质上同一的信息统计了两次。

-------------------------------------------------------------------

*贝叶斯分类的好处在于进一步的证据可以在原有基础上使用。*  

**在检测垃圾邮件的例子中，目前需要做的是通过分析带有正确标注信息的电子邮件训练集，发现样本特征与其所属类别之间的联系----该联系称为“模型”。**  

2、“任务”、“模型”、“特征”是机器学习的三大“原料”。如下图所示：  

![1531548572074](ml1)

* 要完成一个任务，需要建立从用特征描述的数据到输出的恰当映射（即模型）。
* 学习问题的中心任务是研究如何从训练数据中获取这样的映射（模型）。
* “任务”和“学习问题”的不同在于：  
  * “任务”是通过模型来完成的。
  * “学习问题”是通过能够产生模型的学习算法来解决的。
  * 也就是学习问题是需要学习算法学习出模型来完成任务。  

> ***机器学习所关注的问题是使用正确的特征来构建正确的模型，以完成既定的任务***。  

--------------------------------------------------------------

## 一 机器学习的构成要素：  

*模型赋予了机器学习领域以多样性，而特征和任务则为其带来了某种程度的一致性。*  

### 任务：可通过机器学习解决的问题    

**任务（task）是对我们所期望解决的、与问题域对象有关的问题的一种抽象表示**。

* 有时完全抛弃离散类别的概念，而预测出具体值更为自然。这样的任务称为----**回归（regression）**。  
  * 本质上是依据标注有函数输出真值的训练样本集来学习一个实值函数。  

分类和回归的共同假设是可以获得由带有类别真值或函数真值标注的样本所构成的训练集。这样的学习称之为----**有监督学习（supervised learning）**。  

而相对应的对数据进行分组，但不利用与“组”有关的任何先验信息的任务称为**聚类（clustering）**，这样的学习称之为----**无监督学习（unsurpervised learning）**。  

* 通过无监督学习的方式，可以从数据中学到许多其他类型的模式。如*关联规则（association）*。    
* 按照模型的输出是否含有目标变量来划分模型：  
  * 预测性模型（predicative model）  
  * 描述性模型（descriptive model）  

机器学习分类可用下表来表示：  

|            | 预测性模型 |           描述性模型           |
| :--------: | :--------: | :----------------------------: |
| 有监督学习 | 分类、回归 | 子群发现（subgroup discovery） |
| 无监督学习 | 预测性聚类 |    描述性聚类、关联规则发现    |



### 模型：机器学习的输出    

对三组模型进行讨论：  

* 几何模型（geometric model）
* 概率模型（probabilistic model）
* 逻辑模型（logical model）  

#### 几何模型：

* **实例空间（instance space）**：所以可能的或可描述的实例（即样本）所构成的集合。
* 几何模型则是借助于一些几何概念（如线、平面及距离）直接在实例空间中构建的。
* **线性可分（linearly seperable）**：如果存在某个线性决策面能够将两类样本分离，称所给的数据集为线性可分的。  
* **距离（distance）**：如果两个实例之间的距离很小，则意味着二者的特征值相似。在笛卡尔坐标系中，距离可用*欧式距离（Euclidean distance）*来度量：$\sqrt{\sum^d_{i=1}(x_i-y_i)^2}$。  
  * 最近邻分类器（nearest-neighbor classifier）：为确定一个新实例所属的类别，可以首先从内存中获取与该实例最相似的训练实例（即与待分类实例欧式距离最小的那些训练实例），并将这些训练实例的类别赋予该实例）。

#### 概率模型：  

比如贝叶斯分类器。  

* 令$X$为已知变量（如实例的特征）。
* $Y$为我们感兴趣的**目标变量（target variable）**（如实例所属的类别）。
* 机器学习最关键的问题是如何对X和Y之间的以来关系进行建模。-----*在统计学中，假设这些变量的观测值由一些潜在的随机过程按照某个明确定义、但却未知的概率分布所产生。*希望通过数据来获得与该分布有关的更多信息。  

> “后验概率（posterior probability）”---条件概率的使用发生在观测到特征X之后。    

* 依据X和后验概率$P(Y|X)$来预测Y的方法称为**决策规则（decision rule）。    

> “先验概率（prior probability）”---P(Y)。  在分类问题中，其传达的信息是在观测到数据X之前每个类别的数据出现的可能性。
>
> “似然函数”---$P(X|Y)$  
>
> 三者关系：（通过贝叶斯公式）  
>
> $$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$$  
>
> “最大后验概率（Maximum A Posteriori，MAP）决策规则”：  
>
> $$y_{MAP}=arg max_YP(Y|X)=arg max_Y\frac{P(X|Y)P(Y)}{P(X)}=arg max_YP(X|Y)P(Y)$$  
>
> 如果先验分布为均匀分布，则上式可以简化为“最大似然（Maximum Likelihood，ML）决策规则：  
>
> $$y_{MAP}=arg max_YP(X|Y)$$    
>
> **如果忽略先验分布或假定他为均匀分布，则使用似然，否则使用后验概率。**  
>
> 如果问题域中仅涉及两个类别，则使用后验概率比或似然比会更方便。
>
> 

#### 逻辑模型：  

这种模型很容易使用一个树形结构来表示----特征树（feature tree）---利用特征以迭代的当时不短划分实例空间。



### 特征：机器学习的马达  

模型的质量直接由特征决定的，特征可被视为一种易于在任意实例上度量的测度。  

* 特征本质上是一个从实例空间到由特征的值所构成的集合（即特征的域）的映射。

![1531728123858](MLM.png)

上图每一行对应于一个实例，每一列对应于一个特征。  

* 特征和模型的关系非常密切，模型是由特征而定义的，而且还可以将单个特征转化为**单变量模型（univariate model）**  

#### 特征的两种用法（与区分分组模型与评分模型相呼应）：    

* 第一种重点关注实例空间中的某个特定区域（特别在逻辑模型中），也就是说以某种方法对实例空间进行划分---**作为划分的特征**： 
  * 满足**二元分裂（binary split）**  条件：将实例空间分为两组：
    * 一组满足给定条件
    * 一组则不满足
  * 或者非二元分裂也可行  
* 第二种集中地出现在监督学习中----**作为预测器的特征**。  
  * *特征对于模型的最终决策所做的贡献既精确又可度量。*  
  * 决策时，每一维特征并未被“阈值化”，其“分辨率”被完整地运用于计算实例的得分。  

>  有时他们会同时出现在同一个模型中。    

#### 特征的构造（construction）与变换：    

* 在实值特征中通常包含一些通过**离散化（discretidation）**便可以去除的无用细节。  

  * 在分类任务中，可以通过离散化来提高某个特征的**信噪比（signal-to-noise ratio，SNR）。  

* 通过增加一个第三维的特征，可以得到一个值得关注的技巧：*无须实质上去构建新的特征空间，便可获得该特征空间的分类器。如下图所示，左侧的数据线性不可分，右侧将实例空间映射到一个新的由原始特征的平方构造的新“特征空间”后，这组数据几乎线性可分了：  

  ![1531729593143](linear.png)

  

## 二 两类分类及相关任务  

* 机器学习中的对象通常被称为**实例（instance）**。  
* 所有可能的实例所构成的集合称为**实例空间（instance space）**。  
* 带有**标签（label）**的实例---**样本**。------构成了**训练集（training set）**。    

下图为预测性机器学习场景：

![1531732067110](1234444.png)

* 数据中可能会存在噪声：  
  * **标签噪声（label noise）**：所观察到的标签携带了噪声。 
  * **实例噪声（instance noise）**：四观测到的实例因某种原因偏离了真实值。  

  

### 分类：  

*分类器的目的在于构造一个函数预测，以使他尽可能地逼近真实类别。*  

* 最简单的情形下，问题域中仅涉及两个类别-----**二元分类（binary classification）**或**概念学习（concept learning）**。  

#### 分类性能的评价：  

可用**列联表（contingency table）**和**混淆矩阵（confusion meatrix）**来概括。  

* **准确率（accuracy）**是最简单的一种指标，所度量的是测试实例中被正确分类的比例。  
* **错误率（error rate）**即被错误分类的实例在整个测试集中所占的比例。    
* **灵敏度（sensitivity）**或者**特异度（specificity）**：
  * **真正率**----对任意正例被正确分类的概率的一个估计。  
  * **真负率**---对任意负例被正确分类的概率的一个估计。    
* 如果感兴趣的类为少数类，且其规模很小，则不应针对多数类的准确率和性能。  
  * 使用**精度（precision）**取代真负率。-----在这预测为正类的实例中实际正例所占的比例。

下图为分类器性能评价指标：  

![1531733878433](accesss.png)